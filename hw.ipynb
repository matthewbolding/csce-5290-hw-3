{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from torch import Tensor\n",
    "import math\n",
    "\n",
    "# Initializing data...\n",
    "x_i = np.array([[ 0.12, -0.87,  0.33,  0.45],\n",
    "                [ 0.76,  0.21, -0.34,  0.67],\n",
    "                [-0.55,  0.18,  0.29, -0.73],\n",
    "                [ 0.03, -0.99,  0.42,  0.11],\n",
    "                [ 0.76,  0.21, -0.34,  0.67],\n",
    "                [-0.31,  0.66, -0.74,  0.09],\n",
    "                [-0.92,  0.37,  0.28, -0.50]])\n",
    "\n",
    "h_e = np.array([[ 0.64, -0.27,  0.89, -0.12],\n",
    "                [-0.45,  0.33,  0.71,  0.08],\n",
    "                [ 0.19, -0.94,  0.56,  0.37],\n",
    "                [ 0.03,  0.85, -0.41,  0.76],\n",
    "                [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0]])\n",
    "\n",
    "h_d = np.array([[0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0], [0, 0, 0, 0],\n",
    "                [0.58, -0.13,  0.94, 0.22],\n",
    "                [0.45,  0.11, -0.88, 0.67],\n",
    "                [0, 0, 0, 0]])\n",
    "\n",
    "# Casting to Tensors...\n",
    "x_i_tensor = torch.from_numpy(x_i)\n",
    "h_d_tensor = torch.from_numpy(h_d)\n",
    "h_e_tensor = torch.from_numpy(h_e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "Calculate the score of two vectors.\n",
    "'''\n",
    "def score(h_d: Tensor, h_e: Tensor):\n",
    "    num = torch.dot(h_d, h_e)\n",
    "    dim = h_d.size(dim=0)\n",
    "    denom = torch.sqrt(torch.as_tensor([dim]))\n",
    "\n",
    "    return torch.div(num, denom)\n",
    "\n",
    "'''\n",
    "Calculate the attention matrix.\n",
    "'''\n",
    "def calculate_attn(x_i_tensor: Tensor, h_d_tensor: Tensor, h_e_tensor: Tensor):\n",
    "    x_i_size = x_i_tensor.size(dim=0)\n",
    "    a = torch.zeros((x_i_size, x_i_size))\n",
    "    for i in range(1, x_i_size):\n",
    "        for j in range(x_i_size):\n",
    "            h_d_index = i - 1\n",
    "            h_e_index = j\n",
    "            a[i][j] = score(h_d_tensor[h_d_index], h_e_tensor[h_e_index])\n",
    "    \n",
    "    return torch.softmax(a, dim=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429],\n",
      "        [0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429, 0.1429],\n",
      "        [0.2181, 0.1436, 0.1807, 0.1016, 0.1187, 0.1187, 0.1187],\n",
      "        [0.1065, 0.0997, 0.1264, 0.2349, 0.1441, 0.1441, 0.1441]])\n"
     ]
    }
   ],
   "source": [
    "a = calculate_attn(x_i_tensor, h_d_tensor, h_e_tensor)\n",
    "print(a)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.0544, 0.0850, 0.1400, 0.2205]])\n"
     ]
    }
   ],
   "source": [
    "def contribution(i: int, a: Tensor, h_e_tensor: Tensor):\n",
    "    c_i_dim = h_e_tensor.size(dim=1)\n",
    "    x_i_size = h_e_tensor.size(dim=0)\n",
    "\n",
    "    c_i = torch.zeros((1, c_i_dim))\n",
    "\n",
    "    for j in range(x_i_size):\n",
    "        c_i += a[i][j] * h_e_tensor[j]\n",
    "        \n",
    "    return c_i\n",
    "\n",
    "c_i_traduction = contribution(6, a, h_e_tensor)\n",
    "print(c_i_traduction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "W_Q = np.array([[ 0.12, -0.87,  0.33,  0.45],\n",
    "                [ 0.76,  0.21, -0.34,  0.67],\n",
    "                [-0.55,  0.18,  0.29, -0.73],\n",
    "                [ 0.03, -0.99,  0.42,  0.11]])\n",
    "\n",
    "W_K = np.array([[ 0.64, -0.27,  0.89, -0.12],\n",
    "                [-0.45,  0.33,  0.71,  0.08],\n",
    "                [ 0.19, -0.94,  0.56,  0.37],\n",
    "                [ 0.03,  0.85, -0.41,  0.76]])\n",
    "\n",
    "W_V = np.array([[ 0.58, -0.13,  0.94,  0.22 ],\n",
    "                [-0.31,  0.66, -0.74,  0.09],\n",
    "                [ 0.45,  0.11, -0.88,  0.67],\n",
    "                [-0.92,  0.37,  0.28, -0.50]])\n",
    "\n",
    "W_Q_tensor = torch.from_numpy(W_Q)\n",
    "W_K_tensor = torch.from_numpy(W_K)\n",
    "W_V_tensor = torch.from_numpy(W_V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5\n",
      "tensor([[-0.0278,  0.0325, -0.3108,  0.0412],\n",
      "        [ 0.0488, -0.1117,  0.0405,  0.0265],\n",
      "        [-0.2811,  0.1322,  0.1575, -0.2035],\n",
      "        [-0.0825,  0.0691, -0.2955,  0.0031],\n",
      "        [ 0.0488, -0.1117,  0.0405,  0.0265],\n",
      "        [-0.0947, -0.0424,  0.3061, -0.1067],\n",
      "        [-0.2932,  0.1361,  0.1879, -0.2166]], dtype=torch.float64)\n"
     ]
    }
   ],
   "source": [
    "Q_tensor = torch.matmul(x_i_tensor, W_Q_tensor)\n",
    "K_tensor = torch.matmul(x_i_tensor, W_K_tensor)\n",
    "V_tensor = torch.matmul(x_i_tensor, W_V_tensor)\n",
    "\n",
    "def scaled_dot_product_attention(Q: Tensor, K: Tensor, V: Tensor, d: int):\n",
    "    scaling_factor = 1 / math.sqrt(d)\n",
    "    print(scaling_factor)\n",
    "\n",
    "    attn_weight = torch.matmul(Q, K.T) * scaling_factor\n",
    "    attn_weight = torch.softmax(attn_weight, dim=1)\n",
    "    return torch.matmul(attn_weight, V)\n",
    "\n",
    "print(scaled_dot_product_attention(Q_tensor, K_tensor, V_tensor, 4))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
