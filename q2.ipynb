{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cc4149c",
   "metadata": {},
   "source": [
    "## Question 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "91f2cf05",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0278,  0.0325, -0.3108,  0.0412],\n",
      "        [ 0.0488, -0.1117,  0.0405,  0.0265],\n",
      "        [-0.2811,  0.1322,  0.1575, -0.2035],\n",
      "        [-0.0825,  0.0691, -0.2955,  0.0031],\n",
      "        [ 0.0488, -0.1117,  0.0405,  0.0265],\n",
      "        [-0.0947, -0.0424,  0.3061, -0.1067],\n",
      "        [-0.2932,  0.1361,  0.1879, -0.2166]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch import Tensor\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "X_i = torch.tensor([[ 0.12, -0.87,  0.33,  0.45],\n",
    "                    [ 0.76,  0.21, -0.34,  0.67],\n",
    "                    [-0.55,  0.18,  0.29, -0.73],\n",
    "                    [ 0.03, -0.99,  0.42,  0.11],\n",
    "                    [ 0.76,  0.21, -0.34,  0.67],\n",
    "                    [-0.31,  0.66, -0.74,  0.09],\n",
    "                    [-0.92,  0.37,  0.28, -0.50]])\n",
    "\n",
    "W_Q = torch.tensor([[ 0.12, -0.87,  0.33,  0.45],\n",
    "                    [ 0.76,  0.21, -0.34,  0.67],\n",
    "                    [-0.55,  0.18,  0.29, -0.73],\n",
    "                    [ 0.03, -0.99,  0.42,  0.11]])\n",
    "\n",
    "W_K = torch.tensor([[ 0.64, -0.27,  0.89, -0.12],\n",
    "                    [-0.45,  0.33,  0.71,  0.08],\n",
    "                    [ 0.19, -0.94,  0.56,  0.37],\n",
    "                    [ 0.03,  0.85, -0.41,  0.76]])\n",
    "\n",
    "W_V = torch.tensor([[ 0.58, -0.13,  0.94,  0.22 ],\n",
    "                    [-0.31,  0.66, -0.74,  0.09],\n",
    "                    [ 0.45,  0.11, -0.88,  0.67],\n",
    "                    [-0.92,  0.37,  0.28, -0.50]])\n",
    "\n",
    "Q = torch.matmul(X_i, W_Q)\n",
    "K = torch.matmul(X_i, W_K)\n",
    "V = torch.matmul(X_i, W_V)\n",
    "\n",
    "def scaled_dot_product_attention(Q: Tensor, K: Tensor, V: Tensor, d: int):\n",
    "    scaling_factor = 1 / math.sqrt(d)\n",
    "\n",
    "    attn_weight = torch.matmul(Q, K.T) * scaling_factor\n",
    "    attn_weight = torch.softmax(attn_weight, dim=1)\n",
    "    return torch.matmul(attn_weight, V)\n",
    "\n",
    "print(scaled_dot_product_attention(Q, K, V, 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "c69cd1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.0052,  0.0082],\n",
      "        [ 0.1272, -0.1640],\n",
      "        [-0.3544,  0.1830],\n",
      "        [-0.0876,  0.0620],\n",
      "        [ 0.1272, -0.1640],\n",
      "        [-0.1071, -0.0197],\n",
      "        [-0.3686,  0.1901]])\n",
      "tensor([[-0.0302, -0.0556],\n",
      "        [ 0.1970, -0.1066],\n",
      "        [-0.0299, -0.0477],\n",
      "        [-0.0742, -0.0441],\n",
      "        [ 0.1970, -0.1066],\n",
      "        [ 0.1544, -0.0830],\n",
      "        [-0.0332, -0.0460]])\n"
     ]
    }
   ],
   "source": [
    "W_Q_1, W_Q_2 = torch.chunk(W_Q, 2, dim=1)\n",
    "W_K_1, W_K_2 = torch.chunk(W_K, 2, dim=1)\n",
    "W_V_1, W_V_2 = torch.chunk(W_V, 2, dim=1)\n",
    "\n",
    "head_1 = scaled_dot_product_attention(X_i @ W_Q_1, X_i @ W_K_1, X_i @ W_V_1, 2)\n",
    "head_2 = scaled_dot_product_attention(X_i @ W_Q_2, X_i @ W_K_2, X_i @ W_V_2, 2)\n",
    "\n",
    "print(head_1)\n",
    "print(head_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
